{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST import all the necessary libraries and modules!\n",
    "import cv2               # import OpenCV\n",
    "import numpy as np       # import NumPy\n",
    "\n",
    "# import instructor made functions \n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "from utils import *      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Detection Lab\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Let's create a program to <b style=\"color:magenta\">track an object</b> (ie. Traffic Signs) using its <b style=\"color:magenta\">features</b> in a video!\n",
    "    </p> \n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    In this lab, we will learn how to: \n",
    "    <ul style='font-size:1.75rem;line-height:2'>\n",
    "        <li><b style=\"color:blue\">DRAW features</b> on images:\n",
    "            <br> <code>cv2.drawKeypoints</code></li>\n",
    "        <li><b style=\"color:green\">DETECT features</b> of an image using <b style=\"color:green\">three different</b> feature detection algorithms: \n",
    "            <br> <code>SIFT</code>, <code>SURF</code>, and <code>ORB</code></li>\n",
    "        <li><b style=\"color:orange\">FIND matches</b> between images: \n",
    "            <br><code>FLANN</code></li>\n",
    "    </ul>\n",
    "    </p> \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Features?\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:blue\">Features (keypoints)</b> are <b style=\"color:blue\">distinct and easily recognizable</b> regions of an image. \n",
    "    </p>\n",
    "\n",
    "## Drawing Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To <b style=\"color:blue\">draw keypoints</b> on an image, use <code>cv2.drawKeypoints</code>. \n",
    "    <br>It has the following format:\n",
    "    </p>\n",
    "\n",
    "```python\n",
    "cv2.drawKeypoints(<image>, <keypoints>, <image>, flags=5)\n",
    "```\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Here is an example of keypoints (the circles) drawn on an image of a building. \n",
    "    </p>\n",
    "\n",
    "<img src=\"building_keypoints.jpg\" alt=\"building_keypoints\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    We will be introducing <b style=\"color:green\">three different</b> feature detection functions:\n",
    "    <br><code>SIFT</code>, <code>SURF</code>, and <code>ORB</code>\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    For more information on SIFT, SURF, and ORB, check out the article below!\n",
    "    </p>\n",
    "\n",
    "https://pysource.com/2018/03/21/feature-detection-sift-surf-obr-opencv-3-4-with-python-3-tutorial-25/\n",
    "\n",
    "\n",
    "### SIFT\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    SIFT can detect features in an image <b style=\"color:green\">regardless of size or scale</b>.\n",
    "    </p>\n",
    "    \n",
    "<img src=\"TaylorSIFT.jpg\" alt=\"TaylorSIFT\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To get an <b style=\"color:green\">image's features using SIFT</b>, use <code>sift.detectAndCompute</code>. \n",
    "    <br> This function can only take in <b style=\"color:green\">grayscale images</b>.\n",
    "    </p> \n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p> \n",
    "\n",
    "```python\n",
    "keypoints, descriptors = sift.detectAndCompute(<grayscale_image>, None)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Get the <b>features</b> from <code>building.jpg</code> using <b>SIFT</b>.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fe2110ddcb5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# TASK #6: Close the window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Print the number of keypoints detected!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adrita/Documents/online-labs-master/utils.pyc\u001b[0m in \u001b[0;36mclose_windows\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m'''Close popup window via 'ESC' key.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TASK #1: Read \"building.jpg\"\n",
    "\n",
    "building = cv2.imread('building.jpg')\n",
    "\n",
    "# TASK #2: Convert BGR image to grayscale via cv2.cvtColor\n",
    "\n",
    "building_gray = cv2.cvtColor(building, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a SIFT object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# TASK #3: Get keypoints of the grayscale image via sift.detectAndCompute\n",
    "\n",
    "keypoints, descriptors = sift.detectAndCompute(building_gray, None)\n",
    "\n",
    "# TASK #4: Draw the keypoints onto the image via cv2.drawKeypoints\n",
    "\n",
    "building_keys = cv2.drawKeypoints(building_gray, keypoints, building, flags=5)\n",
    "\n",
    "# TASK #5: Show the image via cv2.imshow\n",
    "\n",
    "cv2.imshow('building', building_keys)\n",
    "\n",
    "# TASK #6: Close the window\n",
    "\n",
    "close_windows()\n",
    "\n",
    "# Print the number of keypoints detected!\n",
    "print(\"# SIFT Keypoints: {}\".format(len(keypoints)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURF\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    SURF detects features <b style=\"color:magenta\">faster than SIFT</b>\n",
    "    </p>\n",
    "    \n",
    "<img src=\"BetterSurf.png\" alt=\"BetterSurf\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To get an <b style=\"color:magenta\">image's features using SURF</b>, there are two steps: \n",
    "    <ul style='font-size:1.75rem;line-height:1.5'>\n",
    "      <li>Set <code>hessianThreshold</code> to <b style=\"color:green\">control the number of features found</b> in an image.\n",
    "          <br> <b style=\"color:green\">Threshold values</b> are usually between <code>300</code> to <code>500</code>. \n",
    "          <br><b style=\"color:green\">Smaller thresholds create more keypoints</b>.</li>\n",
    "        <br>\n",
    "        <li><b style=\"color:blue\">Get SURF features</b> using <code>surf.detectAndCompute</code>. \n",
    "            <br>This function can only take in <b style=\"color:blue\">grayscale images</b>. </li>\n",
    "     </ul>\n",
    "     </p>\n",
    "     \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p> \n",
    "\n",
    "```python\n",
    "keypoints, descriptors = surf.detectAndCompute(<grayscale_image>, None)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Get the <b>features</b> from <code>building.jpg</code> using <b>SURF</b>.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5dbada5f4fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# TASK #7: Close the window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adrita/Documents/online-labs-master/utils.pyc\u001b[0m in \u001b[0;36mclose_windows\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m'''Close popup window via 'ESC' key.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TASK #1: Read \"building.jpg\"\n",
    "\n",
    "building1 = cv2.imread('building.jpg')\n",
    "\n",
    "# TASK #2: Convert BGR image to grayscale via cv2.cvtColor\n",
    "\n",
    "building_gray1 = cv2.cvtColor(building1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# TASK #3: Set hessianThreshold to your desired value (bigger = less keypoints)\n",
    "hessianThreshold = 5000      # Change this number!\n",
    "\n",
    "# Create a SURF object\n",
    "surf = cv2.xfeatures2d.SURF_create(hessianThreshold, extended=True)\n",
    "\n",
    "# TASK #4: Get keypoints of the grayscale image via surf.detectAndCompute\n",
    "\n",
    "keypoints1, descriptors1 = surf.detectAndCompute(building_gray, None)\n",
    "\n",
    "# TASK #5: Draw the keypoints onto the image via cv2.drawKeypoints\n",
    "\n",
    "building_key1 = cv2.drawKeypoints(building_gray1, keypoints1, building1, flags=5)\n",
    "\n",
    "# TASK #6: Show the image via cv2.imshow\n",
    "\n",
    "cv2.imshow('building1', building_key1)\n",
    "\n",
    "# TASK #7: Close the window\n",
    "\n",
    "close_windows()\n",
    "\n",
    "\n",
    "# Print the number of keypoints detected, and the hessian_threshold\n",
    "print(\"# SURF Keypoints: {}, hessianThreshold = {}\".format(len(keypoints), hessianThreshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORB\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    ORB detects <b style=\"color:orange\">less features</b> than SIFT and SURF, but <b style=\"color:orange\">ORB is faster</b>.\n",
    "    </p>\n",
    "    \n",
    "<img src=\"Orbz.png\" alt=\"Orbz\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To get an <b style=\"color:orange\">image's features using ORB</b>, there are two steps: \n",
    "    <ul style='font-size:1.75rem;line-height:1.5'>\n",
    "      <li>Set <code>nFeatures</code> to <b style=\"color:green\">set the maximum number of features found</b> in the image.\n",
    "          <br> The <b style=\"color:green\">default value</b> is <code>500</code></li>\n",
    "        <br>\n",
    "        <li><b style=\"color:blue\">Get ORB features</b> using <code>orb.detectAndCompute</code>. \n",
    "            <br>This function can only take in <b style=\"color:blue\">grayscale images</b>. </li>\n",
    "     </ul>\n",
    "     </p>\n",
    "     \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p> \n",
    "\n",
    "```python\n",
    "keypoints, descriptors = orb.detectAndCompute(<grayscale_image>, None)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Get the <b>features</b> from <code>building.jpg</code> using <b>ORB</b>.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-09af6c6c5bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# TASK #7: Close the window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adrita/Documents/online-labs-master/utils.pyc\u001b[0m in \u001b[0;36mclose_windows\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m'''Close popup window via 'ESC' key.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TASK #1: Read \"building.jpg\"\n",
    "\n",
    "building2 = cv2.imread('building.jpg')\n",
    "\n",
    "# TASK #2: Convert BGR image to grayscale via cv2.cvtColor\n",
    "\n",
    "building_gray2 = cv2.cvtColor(building2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# TASK #3: Set the maximum number of features detected via nFeature\n",
    "nFeatures = 500       # Change this number!\n",
    "\n",
    "# Create an ORB object\n",
    "orb = cv2.ORB_create(nFeatures)\n",
    "\n",
    "# TASK #4: Get keypoints of the grayscale image via surf.detectAndCompute\n",
    "\n",
    "keypoints2, descriptor2 = orb.detectAndCompute(building_gray, None)\n",
    "\n",
    "# TASK #5: Draw the keypoints onto the image via cv2.drawKeypoints\n",
    "\n",
    "building_key2 = cv2.drawKeypoints(building_gray2, keypoints2, building2, flags=5)\n",
    "\n",
    "# TASK #6: Show the image via cv2.imshow\n",
    "\n",
    "cv2.imshow('building2', building_key2)\n",
    "\n",
    "# TASK #7: Close the window\n",
    "\n",
    "close_windows()\n",
    "\n",
    "\n",
    "# Print the number of keypoints detected, and the nFeatures\n",
    "print(\"# ORB Keypoints: {}, nFeatures = {}\".format(len(keypoints), nFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Traffic Signs\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Let's create a program that will <b style=\"color:green\">detect and track a one way sign</b> in a live video stream.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parameters\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Set the <b style=\"color:blue\">three parameters</b> listed below:\n",
    "    <br> \n",
    "    <ol style='font-size:1.75rem;line-height:1.5'>\n",
    "        <li>Set <code>hessianThreshold</code> (used in SURF) to your desired value (bigger = less keypoints). \n",
    "            <br> The ideal value for the hessian threshhold is between <code>300</code> and <code>500</code>.</li>\n",
    "        <br>\n",
    "        <li>Set <code>nFeatures</code> (used in ORB) to set the maximum number of features found in the image.</li>\n",
    "        <br>\n",
    "        <li><code>MIN_MATCH_COUNT</code> determines the minimum number of matches for an object to be considered detected. We recommend that you set it to <code>20</code>.</li>\n",
    "    </ol>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessianThreshold: 5000, nFeatures: 300, MIN_MATCH_COUNT: 1\n"
     ]
    }
   ],
   "source": [
    "# TASK: Define hessianThreshold, nFeature, and MIN_MATCH_COUNT\n",
    "hessianThreshold = 5000  # SIFT\n",
    "nFeatures = 300      # ORB\n",
    "MIN_MATCH_COUNT = 1\n",
    "\n",
    "print('hessianThreshold: {}, nFeatures: {}, MIN_MATCH_COUNT: {}'.format(hessianThreshold, nFeatures, MIN_MATCH_COUNT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Detecting Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    The <b style=\"color:blue\">one way sign</b> below is the sign that we want to <b style=\"color:blue\">detect in the video stream.</b>\n",
    "    </p>\n",
    "    \n",
    "<img src=\"one_way.png\" alt=\"one_way\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> As you've probably noticed, finding features between SIFT, SURF, and ORB got <b>kind of repetitive</b>. \n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    We've created <code>find_keypoints</code> function so that we can more easily find keypoints between the three algorithms. <b>Help us finish it!</b>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keypoints(img, feature_detection_algorithm):\n",
    "    ''' Find keypoints for image using selected feature detection algorithm.\n",
    "        Inputs: filename (string): RGB image filepath\n",
    "                feature_detection_algorithm (string): \"sift\", \"surf\", or \"orb\" \n",
    "        Outputs: keypoints (list)\n",
    "        '''\n",
    "    # TASK #1: Convert BGR image to grayscale via cv2.cvtColor\n",
    "    #          Save the grayscale image into the variable 'img_grayscale'\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Shrink image to reduce keypoints for better run time\n",
    "    def shrink_image(image):\n",
    "        image = cv2.resize(image, None, fx=0.2, fy=0.2, interpolation=cv2.INTER_AREA)\n",
    "        return image\n",
    "    shrink_image(img)\n",
    "    shrink_image(img_gray)\n",
    "\n",
    "    # Create SIFT, SURF, or ORB objects\n",
    "    if feature_detection_algorithm=='sift':\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        # TASK #2: Detect SIFT keypoints and descriptors\n",
    "        \n",
    "        keypoints, descriptors = sift.detectAndCompute(img_gray, None)\n",
    "        \n",
    "    elif feature_detection_algorithm=='surf':\n",
    "        surf = cv2.xfeatures2d.SURF_create(hessianThreshold, extended=True) \n",
    "        # TASK #3: Detect SURF keypoints and descriptors\n",
    "        \n",
    "        keypoints, descriptors = surf.detectAndCompute(img_gray, None)\n",
    "        \n",
    "    elif feature_detection_algorithm=='orb':\n",
    "        orb = cv2.ORB_create(nFeatures)\n",
    "        # TASK #4: Detect ORB keypoints and descriptors\n",
    "        \n",
    "        keypoints, descriptors = orb.detectAndCompute(img_gray, None)\n",
    "\n",
    "    # return keypoints and descriptors\n",
    "    return (keypoints, descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Let's <b>test out the function</b> we just created! <b>Make sure you run the cell block above.</b>\n",
    "    <ol style='font-size:1.75rem;line-height:2'>\n",
    "        <li><b style=\"color:blue\">Read</b> <code>one_way.png</code> as img, and use either <code>\"sift\"</code>, <code>\"surf\"</code>, or <code>\"orb\"</code> as the feature detection algorithm.</li>\n",
    "        <li>Play around and try to <b style=\"color:blue\">find the differences</b> between the three algorithms! </li>\n",
    "        <li><b style=\"color:blue\">Tweak</b> the <code>hessianThreshold</code>, <code>nFeatures</code>, and <code>MIN_MATCH_COUNT</code> parameters above for different results!</li>\n",
    "    </ol>\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    You should get something that looks like this:\n",
    "    </p>\n",
    "\n",
    "<img src=\"one_way_features.png\" alt=\"one_way_features\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-72ee36af2457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawKeypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} keypoints\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_detection_algorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Print the number of keypoints detected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adrita/Documents/Racecar_MIT_MS_Program/utils.py\u001b[0m in \u001b[0;36mclose_windows\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclose_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m'''Close popup window via 'ESC' key.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TASK #1: Read \"one_way.png\" using cv2.imread. \n",
    "#          Save as 'img'. \n",
    "\n",
    "img = cv2.imread('one_way.png')\n",
    "\n",
    "# TASK #2: Specify feature detection: \"sift\", \"surf\", \"orb\". \n",
    "#          Save as 'feature_detection_algorithm'.\n",
    "\n",
    "feature_detection_algorithm = \"orb\"\n",
    "\n",
    "# Draw keypoints on image. Press 'ESC' to close window.\n",
    "keypoints, descriptors = find_keypoints(img, feature_detection_algorithm)\n",
    "cv2.drawKeypoints(img, keypoints, img, flags=5)\n",
    "cv2.imshow(\"{} keypoints\".format(feature_detection_algorithm), img)\n",
    "close_windows()\n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"# Keypoints: {}\".format(len(keypoints)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Matching Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Now we need to <b style=\"color:blue\">match the keypoints</b> from the one way sign with keypoints from the video frames. We can use use <code>flann.knnMatch</code> to <b style=\"color:blue\">define the number of matches</b> we want.\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p>\n",
    "    \n",
    "```python\n",
    "matches = flann.knnMatch(<description1>, <description2>, k=<num_best_matches>)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Complete the <code>calculate_matches</code> function below by using <code>flann.knnMatch</code>!\n",
    "    </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matches(des_img, des_frame, feature_detection_algorithm):\n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    if feature_detection_algorithm==\"orb\":\n",
    "        FLANN_INDEX_LSH = 6\n",
    "        index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                           table_number = 6, \n",
    "                           key_size = 12,     \n",
    "                           multi_probe_level = 1)\n",
    "    else:\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        \n",
    "    # create FLANN object\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    # if there are matches\n",
    "    if (des_img is not None) and (des_frame is not None):\n",
    "        # TASK #1: Use flann.knnMatch(). Save as \"matches\" \n",
    "        #          Use des_img for <description1>, des_frame for <description 2>\n",
    "        #          Use 2 for <num_best_matches>.\n",
    "        \n",
    "        matches = flann.knnMatch(des_img, des_frame, k = 2)\n",
    "    \n",
    "    # if there are NO matches   \n",
    "    else:\n",
    "        matches = []\n",
    "    \n",
    "    # return list of matches found\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Live Video Matching\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Display the matches in our video!\n",
    "    <ol style='font-size:1.75rem;line-height:2'>\n",
    "        <li><b style=\"color:blue\">Specify</b> <code>img</code> and <code>feature_detection_algorithm</code> </li>\n",
    "        <li><b style=\"color:blue\">Run</b> the code block below</li>\n",
    "    </ol>\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    If all the functions were implemented correctly, you should get something that looks like this:\n",
    "    </p>\n",
    "\n",
    "<img src=\"one_way_matches.png\" alt=\"one_way_matches\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) /Users/travis/build/skvark/opencv-python/opencv/modules/core/src/matmul.cpp:2268: error: (-215:Assertion failed) scn + 1 == m.cols in function 'perspectiveTransform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-eeb95cc23d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# TASK #3: Call video() with display_matches as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mvideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/adrita/Documents/Racecar_MIT_MS_Program/utils.py\u001b[0m in \u001b[0;36mvideo\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Uncomment this line if issues arise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-eeb95cc23d34>\u001b[0m in \u001b[0;36mdisplay_matches\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# homography to find object in frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mfind_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_matches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_matches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_MATCH_COUNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkp_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkp_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgood_matches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# show matches on video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adrita/Documents/Racecar_MIT_MS_Program/utils.py\u001b[0m in \u001b[0;36mfind_object\u001b[0;34m(img, img_q, total_matches, MIN_MATCH_COUNT, kp_img, kp_frame, m, good_matches, color, query_columns)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperspectiveTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mdst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquery_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.2) /Users/travis/build/skvark/opencv-python/opencv/modules/core/src/matmul.cpp:2268: error: (-215:Assertion failed) scn + 1 == m.cols in function 'perspectiveTransform'\n"
     ]
    }
   ],
   "source": [
    "# TASK #1: Read \"one_way.png\" using cv2.imread. \n",
    "#          Save as 'img'. \n",
    "\n",
    "img = cv2.imread('one_way.png')\n",
    "\n",
    "# TASK #2: Specify feature detection to use: \"sift\", \"surf\", \"orb\". \n",
    "#          Save as 'feature_detection_algorithm'.\n",
    "\n",
    "feature_detection_algorithm = \"surf\"\n",
    "\n",
    "def display_matches(frame):\n",
    "    # Find image keypoints \n",
    "    kp_img, des_img = find_keypoints(img, feature_detection_algorithm)\n",
    "    total_keypoints = float(len(kp_img))\n",
    "    \n",
    "    # find frame keypoints\n",
    "    frame = cv2.resize(frame, None, fx=0.7, fy=0.7, interpolation=cv2.INTER_AREA)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    kp_frame, des_frame = find_keypoints(frame, feature_detection_algorithm)\n",
    "    \n",
    "    # calculate matches\n",
    "    matches = calculate_matches(des_img, des_frame, feature_detection_algorithm)\n",
    "        \n",
    "    # draw matches\n",
    "    img_matches, total_matches, m, good_matches = draw_matches(img, frame, total_keypoints, matches, kp_img, kp_frame)\n",
    "    \n",
    "    # homography to find object in frame\n",
    "    find_object(img_matches, img, total_matches, MIN_MATCH_COUNT, kp_img, kp_frame, m, good_matches, (0, 255, 0), img.shape[1])\n",
    "    \n",
    "    # show matches on video\n",
    "    cv2.imshow('video', img_matches)\n",
    "\n",
    "# TASK #3: Call video() with display_matches as input\n",
    "\n",
    "video(display_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
